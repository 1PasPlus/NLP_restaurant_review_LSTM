{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6410731,"sourceType":"datasetVersion","datasetId":3697155}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-11T07:34:50.929622Z","iopub.execute_input":"2023-11-11T07:34:50.930343Z","iopub.status.idle":"2023-11-11T07:34:50.939677Z","shell.execute_reply.started":"2023-11-11T07:34:50.930309Z","shell.execute_reply":"2023-11-11T07:34:50.938804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading Data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/restaurant-reviews/Restaurant reviews.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:34:53.106653Z","iopub.execute_input":"2023-11-11T07:34:53.107441Z","iopub.status.idle":"2023-11-11T07:34:53.179198Z","shell.execute_reply.started":"2023-11-11T07:34:53.107411Z","shell.execute_reply":"2023-11-11T07:34:53.178272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"df = df.drop([\"Restaurant\", \"Reviewer\", \"Metadata\", \"Pictures\" ,\"7514\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:34:55.519727Z","iopub.execute_input":"2023-11-11T07:34:55.520084Z","iopub.status.idle":"2023-11-11T07:34:55.526067Z","shell.execute_reply.started":"2023-11-11T07:34:55.520056Z","shell.execute_reply":"2023-11-11T07:34:55.525117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Time\"] = list(map(lambda data: str(data).split()[0], df[\"Time\"]))\ndf[\"Time\"] = list(map(lambda data: str(data).split(\"/\")[-1], df[\"Time\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:34:59.116147Z","iopub.execute_input":"2023-11-11T07:34:59.116516Z","iopub.status.idle":"2023-11-11T07:34:59.138113Z","shell.execute_reply.started":"2023-11-11T07:34:59.116489Z","shell.execute_reply":"2023-11-11T07:34:59.136051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Rating'] = np.where(df[\"Rating\"] == \"Like\", df['Rating'].value_counts().idxmax(), df['Rating'])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:00.886007Z","iopub.execute_input":"2023-11-11T07:35:00.886399Z","iopub.status.idle":"2023-11-11T07:35:00.895608Z","shell.execute_reply.started":"2023-11-11T07:35:00.886367Z","shell.execute_reply":"2023-11-11T07:35:00.89463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Rating\"] = list(map(lambda data: float(data) >= 3, df[\"Rating\"]))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:02.585153Z","iopub.execute_input":"2023-11-11T07:35:02.58607Z","iopub.status.idle":"2023-11-11T07:35:02.596292Z","shell.execute_reply.started":"2023-11-11T07:35:02.586034Z","shell.execute_reply":"2023-11-11T07:35:02.595284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Time'] = np.where(df[\"Time\"] == \"nan\", df['Time'].value_counts().idxmax(), df['Time'])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:04.233213Z","iopub.execute_input":"2023-11-11T07:35:04.234067Z","iopub.status.idle":"2023-11-11T07:35:04.243138Z","shell.execute_reply.started":"2023-11-11T07:35:04.234036Z","shell.execute_reply":"2023-11-11T07:35:04.242156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmmsTime = MinMaxScaler()\n\nmmsTime.fit(df[[\"Time\"]])\ndf[\"Time\"] = mmsTime.transform(df[[\"Time\"]])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:06.438383Z","iopub.execute_input":"2023-11-11T07:35:06.439078Z","iopub.status.idle":"2023-11-11T07:35:06.45225Z","shell.execute_reply.started":"2023-11-11T07:35:06.439045Z","shell.execute_reply":"2023-11-11T07:35:06.451253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Review\"] = df[\"Review\"].fillna(\"Nothing\")\ndf['Rating'] = df['Rating'].astype(int)\ndf.rename(columns={'Rating': 'target'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:08.359971Z","iopub.execute_input":"2023-11-11T07:35:08.361101Z","iopub.status.idle":"2023-11-11T07:35:08.370332Z","shell.execute_reply.started":"2023-11-11T07:35:08.361058Z","shell.execute_reply":"2023-11-11T07:35:08.369349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus_df(review, target):\n    corpus=[]\n    \n    for x in review[review['target']==target]['Review'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:10.143005Z","iopub.execute_input":"2023-11-11T07:35:10.143461Z","iopub.status.idle":"2023-11-11T07:35:10.148559Z","shell.execute_reply.started":"2023-11-11T07:35:10.143429Z","shell.execute_reply":"2023-11-11T07:35:10.147571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nrestaurant_reviews = create_corpus_df(df, 1)\n\ndic=defaultdict(int)\nfor word in restaurant_reviews:\n    dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\ntop","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:12.352134Z","iopub.execute_input":"2023-11-11T07:35:12.352528Z","iopub.status.idle":"2023-11-11T07:35:12.544225Z","shell.execute_reply.started":"2023-11-11T07:35:12.352499Z","shell.execute_reply":"2023-11-11T07:35:12.543277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\n        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n        '', \n        text\n    )\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:14.662379Z","iopub.execute_input":"2023-11-11T07:35:14.662748Z","iopub.status.idle":"2023-11-11T07:35:14.669177Z","shell.execute_reply.started":"2023-11-11T07:35:14.662717Z","shell.execute_reply":"2023-11-11T07:35:14.668208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# Download the stopwords from NLTK\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:16.80996Z","iopub.execute_input":"2023-11-11T07:35:16.810918Z","iopub.status.idle":"2023-11-11T07:35:17.04345Z","shell.execute_reply.started":"2023-11-11T07:35:16.810881Z","shell.execute_reply":"2023-11-11T07:35:17.042551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\nstemmer = nltk.SnowballStemmer(\"english\")\n\ndef preprocess_data(text):\n    # Clean puntuation, urls, and so on\n    text = clean_text(text)\n    # Remove stopwords and Stemm all the words in the sentence\n    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stop_words)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:18.635709Z","iopub.execute_input":"2023-11-11T07:35:18.636098Z","iopub.status.idle":"2023-11-11T07:35:18.644729Z","shell.execute_reply.started":"2023-11-11T07:35:18.636065Z","shell.execute_reply":"2023-11-11T07:35:18.643808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import  re\nimport string \n\ndf['Review'] = df['Review'].apply(preprocess_data)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:21.149705Z","iopub.execute_input":"2023-11-11T07:35:21.150064Z","iopub.status.idle":"2023-11-11T07:35:28.075423Z","shell.execute_reply.started":"2023-11-11T07:35:21.150035Z","shell.execute_reply":"2023-11-11T07:35:28.074451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus_df(review, target):\n    corpus=[]\n    \n    for x in review[review['target']==target]['Review'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:28.077094Z","iopub.execute_input":"2023-11-11T07:35:28.077418Z","iopub.status.idle":"2023-11-11T07:35:28.083031Z","shell.execute_reply.started":"2023-11-11T07:35:28.077392Z","shell.execute_reply":"2023-11-11T07:35:28.081999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nrestaurant_reviews = create_corpus_df(df, 1)\n\ndic=defaultdict(int)\nfor word in restaurant_reviews:\n    dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\ntop","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:28.084178Z","iopub.execute_input":"2023-11-11T07:35:28.084483Z","iopub.status.idle":"2023-11-11T07:35:28.197988Z","shell.execute_reply.started":"2023-11-11T07:35:28.084459Z","shell.execute_reply":"2023-11-11T07:35:28.197053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nx = df['Review']\ny = df['target']\n\n    # Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:29.900563Z","iopub.execute_input":"2023-11-11T07:35:29.900909Z","iopub.status.idle":"2023-11-11T07:35:29.910099Z","shell.execute_reply.started":"2023-11-11T07:35:29.900883Z","shell.execute_reply":"2023-11-11T07:35:29.909207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost ","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn import metrics\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\npipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        use_label_encoder=False,\n        eval_metric='auc',\n    ))\n])\nfrom sklearn import metrics\n\n# Fit the pipeline with the data\npipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\ny_pred_train = pipe.predict(x_train)\n\nprint('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n\n# Calculate and display the confusion matrix\ncm = confusion_matrix(y_test, y_pred_class)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:31.887794Z","iopub.execute_input":"2023-11-11T07:35:31.888634Z","iopub.status.idle":"2023-11-11T07:35:36.00096Z","shell.execute_reply.started":"2023-11-11T07:35:31.888604Z","shell.execute_reply":"2023-11-11T07:35:35.999885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM ","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn import metrics\nimport lightgbm as lgb  # Importing LightGBM\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Define the pipeline using LightGBM classifier\npipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', lgb.LGBMClassifier(\n        objective='binary',  # or 'multiclass' for multi-class classification\n        metric='auc',\n    ))\n])\n\n# Fit the pipeline with the data\npipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\ny_pred_train = pipe.predict(x_train)\n\nprint('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n\n# Calculate and display the confusion matrix\ncm = confusion_matrix(y_test, y_pred_class)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:39.774583Z","iopub.execute_input":"2023-11-11T07:35:39.775441Z","iopub.status.idle":"2023-11-11T07:35:41.896017Z","shell.execute_reply.started":"2023-11-11T07:35:39.775408Z","shell.execute_reply":"2023-11-11T07:35:41.89507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM ","metadata":{}},{"cell_type":"code","source":"train_reviews = df['Review'].values\ntrain_target = df['target'].values","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:42.695117Z","iopub.execute_input":"2023-11-11T07:35:42.695955Z","iopub.status.idle":"2023-11-11T07:35:42.700136Z","shell.execute_reply.started":"2023-11-11T07:35:42.695926Z","shell.execute_reply":"2023-11-11T07:35:42.699225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the length of our vocabulary\nfrom keras.preprocessing.text import Tokenizer\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(train_reviews)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:44.954068Z","iopub.execute_input":"2023-11-11T07:35:44.954462Z","iopub.status.idle":"2023-11-11T07:35:45.309059Z","shell.execute_reply.started":"2023-11-11T07:35:44.954431Z","shell.execute_reply":"2023-11-11T07:35:45.308116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))\n    \ndef embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:46.801126Z","iopub.execute_input":"2023-11-11T07:35:46.80148Z","iopub.status.idle":"2023-11-11T07:35:46.808484Z","shell.execute_reply.started":"2023-11-11T07:35:46.801455Z","shell.execute_reply":"2023-11-11T07:35:46.807408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Assuming you have already downloaded the NLTK tokenizers\nnltk.download('punkt')\n\nlongest_train = max(train_reviews, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(train_reviews), \n    length_long_sentence, \n    padding='post'\n)\n\ntrain_padded_sentences","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:48.74901Z","iopub.execute_input":"2023-11-11T07:35:48.749439Z","iopub.status.idle":"2023-11-11T07:35:52.097584Z","shell.execute_reply.started":"2023-11-11T07:35:48.749407Z","shell.execute_reply":"2023-11-11T07:35:52.09658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 100\nembeddings_dictionary = dict()\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:52.099187Z","iopub.execute_input":"2023-11-11T07:35:52.099538Z","iopub.status.idle":"2023-11-11T07:35:52.11366Z","shell.execute_reply.started":"2023-11-11T07:35:52.099502Z","shell.execute_reply":"2023-11-11T07:35:52.112788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    train_padded_sentences, \n    train_target, \n    test_size=0.25\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:53.738088Z","iopub.execute_input":"2023-11-11T07:35:53.738741Z","iopub.status.idle":"2023-11-11T07:35:53.751966Z","shell.execute_reply.started":"2023-11-11T07:35:53.738708Z","shell.execute_reply":"2023-11-11T07:35:53.750967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def glove_lstm():\n    model = Sequential()\n    \n    model.add(Embedding(\n        input_dim=embedding_matrix.shape[0], \n        output_dim=embedding_matrix.shape[1], \n        weights = [embedding_matrix], \n        input_length=length_long_sentence\n    ))\n    \n    model.add(Bidirectional(LSTM(\n        length_long_sentence, \n        return_sequences = True, \n        recurrent_dropout=0.2\n    )))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:56.388533Z","iopub.execute_input":"2023-11-11T07:35:56.389475Z","iopub.status.idle":"2023-11-11T07:35:56.39735Z","shell.execute_reply.started":"2023-11-11T07:35:56.38944Z","shell.execute_reply":"2023-11-11T07:35:56.396291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, BatchNormalization, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:35:58.820391Z","iopub.execute_input":"2023-11-11T07:35:58.821316Z","iopub.status.idle":"2023-11-11T07:35:58.825881Z","shell.execute_reply.started":"2023-11-11T07:35:58.821262Z","shell.execute_reply":"2023-11-11T07:35:58.824902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = glove_lstm()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 6,\n    batch_size = 32,\n    validation_data = (X_test, y_test),\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T07:36:00.934796Z","iopub.execute_input":"2023-11-11T07:36:00.935161Z","iopub.status.idle":"2023-11-11T08:52:12.142907Z","shell.execute_reply.started":"2023-11-11T07:36:00.935133Z","shell.execute_reply":"2023-11-11T08:52:12.142018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_learning_curves(history, metrics):\n    plt.figure(figsize=(12, 4))\n\n    for i, metric in enumerate(metrics):\n        plt.subplot(1, len(metrics), i + 1)\n        for m in metric:\n            plt.plot(history.history[m], label=m)\n        plt.title('Model {}'.format(metric[0]))\n        plt.xlabel('Epochs')\n        plt.ylabel(metric[0])\n        plt.legend()\n\n    plt.show()\n\n# これで関数を使用してプロットを表示できます\nplot_learning_curves(history, [['loss', 'val_loss'], ['accuracy', 'val_accuracy']])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:52:12.144757Z","iopub.execute_input":"2023-11-11T08:52:12.145059Z","iopub.status.idle":"2023-11-11T08:52:12.613697Z","shell.execute_reply.started":"2023-11-11T08:52:12.145034Z","shell.execute_reply":"2023-11-11T08:52:12.612683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}